{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original reference material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "print(sys.version)\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class policy_estimator():\n",
    "    def __init__(self, env):\n",
    "        self.n_inputs = env.observation_space.shape[0]\n",
    "        self.n_outputs = env.action_space.n\n",
    "        \n",
    "        # Define network\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.n_inputs, 16), \n",
    "            nn.ReLU(), \n",
    "#             nn.Linear(32, 16),\n",
    "#             nn.ReLU(),        \n",
    "            nn.Linear(16, self.n_outputs),\n",
    "            nn.Softmax(dim=-1))\n",
    "    \n",
    "    def predict(self, state):\n",
    "        action_probs = self.network(torch.FloatTensor(state))\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('CartPole-v0')\n",
    "# env = gym.make(\"Acrobot-v1\")\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env._max_episode_steps = 1000\n",
    "# add in 1000 steps if this doesn't work!\n",
    "\n",
    "s = env.reset()\n",
    "pe = policy_estimator(env)\n",
    "print(pe.predict(s))\n",
    "print(pe.network(torch.FloatTensor(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    r = np.array([gamma**i * rewards[i] \n",
    "                  for i in range(len(rewards))])\n",
    "    # Reverse the array direction for cumsum and then\n",
    "    # revert back to the original order\n",
    "    r = r[::-1].cumsum()[::-1]\n",
    "    return r - r.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reinforce(env, policy_estimator, num_episodes=2000,\n",
    "              batch_size=10, gamma=0.99, lr = 0.01):\n",
    "\n",
    "    # Set up lists to hold results\n",
    "    total_rewards = []\n",
    "    batch_rewards = []\n",
    "    batch_actions = []\n",
    "    batch_states = []\n",
    "    batch_counter = 1\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(policy_estimator.network.parameters(), \n",
    "                           lr=lr)\n",
    "    \n",
    "    action_space = np.arange(env.action_space.n)\n",
    "    for ep in range(num_episodes):\n",
    "        s_0 = env.reset().unsqueeze(0)\n",
    "        states = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        complete = False\n",
    "        while complete == False:\n",
    "            # Get actions and convert to numpy array\n",
    "            action_probs = policy_estimator.predict(s_0).detach().numpy()\n",
    "            action = np.random.choice(action_space, p=action_probs)\n",
    "            s_1, r, complete, _ = env.step(action)\n",
    "            \n",
    "            states.append(s_0)\n",
    "            rewards.append(r)\n",
    "            actions.append(action)\n",
    "            s_0 = s_1\n",
    "            \n",
    "            # If complete, batch data\n",
    "            if complete:\n",
    "                batch_rewards.extend(discount_rewards(rewards, gamma))\n",
    "                batch_states.extend(states)\n",
    "                batch_actions.extend(actions)\n",
    "                batch_counter += 1\n",
    "                total_rewards.append(sum(rewards))\n",
    "                \n",
    "                # If batch is complete, update network\n",
    "                if batch_counter == batch_size:\n",
    "                    optimizer.zero_grad()\n",
    "                    state_tensor = torch.FloatTensor(batch_states)\n",
    "                    reward_tensor = torch.FloatTensor(batch_rewards)\n",
    "                    # Actions are used as indices, must be LongTensor\n",
    "                    action_tensor = torch.LongTensor(batch_actions)\n",
    "                    \n",
    "                    #Moving average baseline\n",
    "                    \n",
    "      \n",
    "                    # Basically create a tensor size [1, len(reward_tensor)] Below doesn't actually work\n",
    "                    WINDOW = 20\n",
    "                    baseline = np.mean(total_rewards[-WINDOW:])\n",
    "            \n",
    "                                  \n",
    "                    # Calculate loss\n",
    "                    reward_with_baseline = reward_tensor - baseline\n",
    "                                 \n",
    "                    logprob = torch.log(\n",
    "                        policy_estimator.predict(state_tensor))\n",
    "                    \n",
    "                    selected_logprobs = reward_with_baseline * \\\n",
    "                        logprob[np.arange(len(action_tensor)), action_tensor]\n",
    "                    \n",
    "                    \n",
    "                    # regular reward (should be higher variance. But how do I calculate this?)\n",
    "#                     selected_logprobs = reward_tensor * \\\n",
    "#                         logprob[np.arange(len(action_tensor)), action_tensor]\n",
    "                    loss = -selected_logprobs.mean()\n",
    "    \n",
    "    \n",
    "#                     policy_estimator.zero_grad()     # zeroes the gradient buffers of all parameters. Prevents the gradients from adding up\n",
    "                    \n",
    "                    # Calculate gradients\n",
    "                    loss.backward()\n",
    "                    # Apply gradients\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    batch_rewards = []\n",
    "                    batch_actions = []\n",
    "                    batch_states = []\n",
    "                    batch_counter = 1\n",
    "                    \n",
    "                # Print running average\n",
    "                print(\"\\rEp: {} Average of last 10: {:.2f}\".format(\n",
    "                    ep + 1, np.mean(total_rewards[-10:])), end=\"\")\n",
    "                \n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env = gym.make(\"Acrobot-v1\")\n",
    "# env = gym.make(\"MountainCar-v0\")\n",
    "# env._max_episode_steps = 1000\n",
    "# add in 1000 steps if this doesn't work!\n",
    "# seed = env.seed()\n",
    "# pe = policy_estimator(env)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(5,1, figsize = (12,40))\n",
    "for run in range(5):\n",
    "    # Create a new environment instance (seed is different at each run)\n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "    env._max_episode_steps = 1000\n",
    "    seed = env.seed()\n",
    "    pe = policy_estimator(env)\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "    rewards = reinforce(env, pe)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'time of run {run} for seed {seed} in minutes: {(end-start) / 60.}')\n",
    "    window = 10\n",
    "    smoothed_rewards = [np.mean(rewards[i-window:i+1]) if i > window \n",
    "                        else np.mean(rewards[:i+1]) for i in range(len(rewards))]\n",
    "    \n",
    "    axs[run].plot(rewards)\n",
    "    axs[run].plot(smoothed_rewards)\n",
    "    axs[run].set_ylabel('Total Rewards')\n",
    "    axs[run].set_xlabel('Episodes')\n",
    "    \n",
    "#     plt.figure(figsize=(12,8))\n",
    "#     plt.plot(rewards)\n",
    "#     plt.plot(smoothed_rewards)\n",
    "#     plt.ylabel('Total Rewards')\n",
    "#     plt.xlabel('Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"policygradient_mountainCar-1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My best implementation (with this guide above as reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n",
      "tensor([-0.5814,  0.0000])\n",
      "tensor(1., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import gym\n",
    "from evaluation import evaluate\n",
    "# For sanity, have set the seed on all possible variables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Input differentiable policy parameterization & Step size\n",
    "STEP_SIZE = 0.01\n",
    "\n",
    "\n",
    "class PolicyEstimator(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(PolicyEstimator,self).__init__()\n",
    "        \n",
    "        self.n_input = env.observation_space.shape[0]\n",
    "        self.n_output = env.action_space.n\n",
    "        self.fc1 = nn.Linear(self.n_input, 16)\n",
    "        self.fc2 = nn.Linear(16, self.n_output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim = -1)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# Test of output\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "state = torch.tensor(env.reset()).float()\n",
    "print(state)\n",
    "pe = PolicyEstimator(env)\n",
    "print(pe(state).sum()) # Softmax that should add up to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000001D3EFC64048>\n"
     ]
    }
   ],
   "source": [
    "# Initialize Policy parameter theta (e.g. to 0 vector) -- currently I am going with assumption that they are already initialized\n",
    "# help(pe.zero_grad) # This is also something that I didn't do. This could be important (to prevent building up of gradients)\n",
    "print(pe.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5891,  0.0000])\n",
      "[0.3804588  0.314339   0.30520222]\n",
      "(1, 0.314339)\n"
     ]
    }
   ],
   "source": [
    "#Pick an action\n",
    "\n",
    "# MAKE SURE that you have things seeded. That likely makes things easier. Randomize the seed that you choose I guess. Is there another way to do it?\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "env.seed(0)\n",
    "\n",
    "def action_choice(policy_estimator, state):\n",
    "    # May cause conflicts with the Maze environment\n",
    "    print(state)\n",
    "    action_probs = policy_estimator(state).detach().numpy()\n",
    "    action = np.random.choice((policy_estimator.n_output), p = action_probs)\n",
    "    idx = action_probs[action]\n",
    "    print(action_probs)\n",
    "    return action, idx\n",
    "\n",
    "\n",
    "state = torch.from_numpy(env.reset()).float()\n",
    "print(action_choice(pe, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1000.0\n",
      "1000\n",
      "1000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "#Pick an action\n",
    "def action_choice(policy_estimator, state):\n",
    "    # May cause conflicts with the Maze environment\n",
    "    action_probs = policy_estimator(state).detach().numpy()\n",
    "    action = np.random.choice((policy_estimator.n_output), p = action_probs)\n",
    "    idx = action_probs[action]\n",
    "    return action, idx\n",
    "\n",
    "\n",
    "# Generate an episode (loop forever [actually until episode is done, because it's continuous])\n",
    "\n",
    "done = False\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env._max_episode_steps = 1000\n",
    "seed = env.seed(0)\n",
    "action_prob_val = []\n",
    "action_list = []\n",
    "state_list = []\n",
    "s_0 = torch.from_numpy(env.reset()).float() # Initial state\n",
    "cumulative_reward = 0\n",
    "reward_list = []\n",
    "while not done:\n",
    "    # Pick an action, and step through environment\n",
    "    action, prob = action_choice(pe, s_0)\n",
    "    \n",
    "    # Append this before. Then you'll get the same size\n",
    "    state_list.append(s_0)\n",
    "    \n",
    "    s_1, reward, done, _ = env.step(action)\n",
    "    cumulative_reward += reward\n",
    "    action_prob_val.append(prob)\n",
    "    reward_list.append(reward)\n",
    "    action_list.append(action)\n",
    "    s_0 = torch.from_numpy(s_1).float() # Make state the next one\n",
    "    if done:\n",
    "        print(cumulative_reward) # for a single episode\n",
    "\n",
    "        \n",
    "print(len(reward_list))\n",
    "print(len(action_list))\n",
    "print(len(state_list))\n",
    "print(len(action_prob_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration Testing portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000, 2])\n",
      "tensor([-1.1868, -1.1573, -1.1572, -1.1570, -1.1567, -1.1564, -1.1858, -1.1854,\n",
      "        -1.1552, -1.1845], grad_fn=<SliceBackward>)\n",
      "tensor([2, 1, 1, 1, 1, 1, 2, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# Integration testing. Goal is to get all the way back to the parameters of NN. The reason the other person re-did all the tensors was because of backprop in Pytorch\n",
    "\n",
    "# Change of plans, will start from the beginning\n",
    "\n",
    "reward_tensor = torch.tensor(reward_list).float()\n",
    "action_tensor = torch.tensor(action_list).long()\n",
    "state_tensor = torch.stack(state_list)\n",
    "\n",
    "print(reward_tensor.shape)\n",
    "print(action_tensor.shape)\n",
    "print(state_tensor.shape)\n",
    "\n",
    "# Goal is to make sure that when estimating, you are simulating the same conditions\n",
    "# I don't think that setting the seed is the right way to do it. I prob should confirm whether I get the same value or not.\n",
    "\n",
    "\n",
    "# Does this need to be seeded?\n",
    "action_output = torch.log(pe(state_tensor))\n",
    "\n",
    "# This is just doing an indexing! It picks the action that was chosen in the run. There is no operation here that pytorch has to track\n",
    "picked_logprob = action_output[np.arange(len(action_tensor)), action_tensor]\n",
    "\n",
    "print(picked_logprob[:10])\n",
    "print(action_tensor[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-98241.0859, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Get the return values as well\n",
    "\n",
    "return_tensor = discount_rewards(reward_tensor)\n",
    "# print(return_tensor[-10:])\n",
    "\n",
    "loss = torch.sum(-return_tensor * picked_logprob)\n",
    "print(loss)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-99.9957, -99.9957, -99.9956, -99.9956, -99.9956, -99.9955, -99.9955,\n",
      "        -99.9954, -99.9954, -99.9953, -99.9953, -99.9952, -99.9952, -99.9951,\n",
      "        -99.9951, -99.9950, -99.9950, -99.9949, -99.9949, -99.9948, -99.9948,\n",
      "        -99.9947, -99.9947, -99.9946, -99.9946, -99.9945, -99.9944, -99.9944,\n",
      "        -99.9943, -99.9943, -99.9942, -99.9941, -99.9941, -99.9940, -99.9940,\n",
      "        -99.9939, -99.9938, -99.9938, -99.9937, -99.9937, -99.9936, -99.9935,\n",
      "        -99.9935, -99.9934, -99.9933, -99.9932, -99.9932, -99.9931, -99.9930,\n",
      "        -99.9930, -99.9929, -99.9928, -99.9928, -99.9927, -99.9926, -99.9925,\n",
      "        -99.9924, -99.9924, -99.9923, -99.9922, -99.9921, -99.9921, -99.9920,\n",
      "        -99.9919, -99.9918, -99.9917, -99.9916, -99.9916, -99.9915, -99.9914,\n",
      "        -99.9913, -99.9912, -99.9911, -99.9910, -99.9909, -99.9908, -99.9907,\n",
      "        -99.9907, -99.9906, -99.9905, -99.9904, -99.9903, -99.9902, -99.9901,\n",
      "        -99.9900, -99.9899, -99.9898, -99.9897, -99.9895, -99.9894, -99.9893,\n",
      "        -99.9892, -99.9891, -99.9890, -99.9889, -99.9888, -99.9887, -99.9885,\n",
      "        -99.9884, -99.9883, -99.9882, -99.9881, -99.9880, -99.9878, -99.9877,\n",
      "        -99.9876, -99.9875, -99.9873, -99.9872, -99.9871, -99.9869, -99.9868,\n",
      "        -99.9867, -99.9865, -99.9864, -99.9863, -99.9861, -99.9860, -99.9858,\n",
      "        -99.9857, -99.9855, -99.9854, -99.9853, -99.9851, -99.9850, -99.9848,\n",
      "        -99.9846, -99.9845, -99.9843, -99.9842, -99.9840, -99.9839, -99.9837,\n",
      "        -99.9835, -99.9834, -99.9832, -99.9830, -99.9828, -99.9827, -99.9825,\n",
      "        -99.9823, -99.9821, -99.9819, -99.9818, -99.9816, -99.9814, -99.9812,\n",
      "        -99.9810, -99.9808, -99.9806, -99.9804, -99.9802, -99.9800, -99.9798,\n",
      "        -99.9796, -99.9794, -99.9792, -99.9790, -99.9788, -99.9786, -99.9783,\n",
      "        -99.9781, -99.9779, -99.9777, -99.9774, -99.9772, -99.9770, -99.9768,\n",
      "        -99.9765, -99.9763, -99.9760, -99.9758, -99.9755, -99.9753, -99.9750,\n",
      "        -99.9748, -99.9745, -99.9743, -99.9740, -99.9738, -99.9735, -99.9732,\n",
      "        -99.9729, -99.9727, -99.9724, -99.9721, -99.9718, -99.9715, -99.9713,\n",
      "        -99.9710, -99.9707, -99.9704, -99.9701, -99.9698, -99.9695, -99.9692,\n",
      "        -99.9688, -99.9685, -99.9682, -99.9679, -99.9676, -99.9672, -99.9669,\n",
      "        -99.9666, -99.9662, -99.9659, -99.9655, -99.9652, -99.9648, -99.9645,\n",
      "        -99.9641, -99.9638, -99.9634, -99.9630, -99.9626, -99.9623, -99.9619,\n",
      "        -99.9615, -99.9611, -99.9607, -99.9603, -99.9599, -99.9595, -99.9591,\n",
      "        -99.9587, -99.9583, -99.9578, -99.9574, -99.9570, -99.9565, -99.9561,\n",
      "        -99.9557, -99.9552, -99.9548, -99.9543, -99.9538, -99.9534, -99.9529,\n",
      "        -99.9524, -99.9519, -99.9514, -99.9510, -99.9505, -99.9500, -99.9494,\n",
      "        -99.9489, -99.9484, -99.9479, -99.9474, -99.9468, -99.9463, -99.9458,\n",
      "        -99.9452, -99.9446, -99.9441, -99.9435, -99.9430, -99.9424, -99.9418,\n",
      "        -99.9412, -99.9406, -99.9400, -99.9394, -99.9388, -99.9382, -99.9375,\n",
      "        -99.9369, -99.9363, -99.9356, -99.9350, -99.9343, -99.9337, -99.9330,\n",
      "        -99.9323, -99.9316, -99.9309, -99.9302, -99.9295, -99.9288, -99.9281,\n",
      "        -99.9274, -99.9266, -99.9259, -99.9251, -99.9244, -99.9236, -99.9229,\n",
      "        -99.9221, -99.9213, -99.9205, -99.9197, -99.9189, -99.9181, -99.9172,\n",
      "        -99.9164, -99.9156, -99.9147, -99.9138, -99.9130, -99.9121, -99.9112,\n",
      "        -99.9103, -99.9094, -99.9085, -99.9075, -99.9066, -99.9057, -99.9047,\n",
      "        -99.9037, -99.9028, -99.9018, -99.9008, -99.8998, -99.8988, -99.8978,\n",
      "        -99.8967, -99.8957, -99.8946, -99.8936, -99.8925, -99.8914, -99.8903,\n",
      "        -99.8892, -99.8881, -99.8869, -99.8858, -99.8847, -99.8835, -99.8823,\n",
      "        -99.8811, -99.8799, -99.8787, -99.8775, -99.8762, -99.8750, -99.8737,\n",
      "        -99.8724, -99.8712, -99.8699, -99.8685, -99.8672, -99.8659, -99.8645,\n",
      "        -99.8631, -99.8618, -99.8604, -99.8589, -99.8575, -99.8561, -99.8546,\n",
      "        -99.8531, -99.8517, -99.8502, -99.8486, -99.8471, -99.8456, -99.8440,\n",
      "        -99.8424, -99.8408, -99.8392, -99.8376, -99.8359, -99.8343, -99.8326,\n",
      "        -99.8309, -99.8292, -99.8275, -99.8257, -99.8240, -99.8222, -99.8204,\n",
      "        -99.8186, -99.8167, -99.8149, -99.8130, -99.8111, -99.8092, -99.8073,\n",
      "        -99.8054, -99.8034, -99.8014, -99.7994, -99.7974, -99.7953, -99.7933,\n",
      "        -99.7912, -99.7890, -99.7869, -99.7848, -99.7826, -99.7804, -99.7782,\n",
      "        -99.7759, -99.7737, -99.7714, -99.7691, -99.7667, -99.7644, -99.7620,\n",
      "        -99.7596, -99.7572, -99.7547, -99.7522, -99.7497, -99.7472, -99.7446,\n",
      "        -99.7421, -99.7395, -99.7368, -99.7342, -99.7315, -99.7288, -99.7260,\n",
      "        -99.7233, -99.7205, -99.7176, -99.7148, -99.7119, -99.7090, -99.7060,\n",
      "        -99.7031, -99.7001, -99.6970, -99.6940, -99.6909, -99.6878, -99.6846,\n",
      "        -99.6814, -99.6782, -99.6750, -99.6717, -99.6684, -99.6650, -99.6616,\n",
      "        -99.6582, -99.6548, -99.6513, -99.6477, -99.6442, -99.6406, -99.6370,\n",
      "        -99.6333, -99.6296, -99.6258, -99.6221, -99.6182, -99.6144, -99.6105,\n",
      "        -99.6065, -99.6026, -99.5986, -99.5945, -99.5904, -99.5863, -99.5821,\n",
      "        -99.5779, -99.5736, -99.5693, -99.5649, -99.5605, -99.5561, -99.5516,\n",
      "        -99.5471, -99.5425, -99.5379, -99.5332, -99.5285, -99.5237, -99.5189,\n",
      "        -99.5141, -99.5092, -99.5042, -99.4992, -99.4941, -99.4890, -99.4839,\n",
      "        -99.4786, -99.4734, -99.4681, -99.4627, -99.4573, -99.4518, -99.4462,\n",
      "        -99.4407, -99.4350, -99.4293, -99.4235, -99.4177, -99.4118, -99.4059,\n",
      "        -99.3999, -99.3938, -99.3877, -99.3815, -99.3752, -99.3689, -99.3626,\n",
      "        -99.3561, -99.3496, -99.3430, -99.3364, -99.3297, -99.3229, -99.3161,\n",
      "        -99.3092, -99.3022, -99.2951, -99.2880, -99.2808, -99.2736, -99.2662,\n",
      "        -99.2588, -99.2513, -99.2438, -99.2361, -99.2284, -99.2206, -99.2127,\n",
      "        -99.2048, -99.1968, -99.1886, -99.1804, -99.1722, -99.1638, -99.1554,\n",
      "        -99.1468, -99.1382, -99.1295, -99.1207, -99.1118, -99.1029, -99.0938,\n",
      "        -99.0846, -99.0754, -99.0660, -99.0566, -99.0471, -99.0375, -99.0277,\n",
      "        -99.0179, -99.0080, -98.9980, -98.9878, -98.9776, -98.9673, -98.9569,\n",
      "        -98.9463, -98.9357, -98.9249, -98.9141, -98.9031, -98.8920, -98.8808,\n",
      "        -98.8695, -98.8581, -98.8466, -98.8349, -98.8232, -98.8113, -98.7993,\n",
      "        -98.7871, -98.7749, -98.7625, -98.7500, -98.7374, -98.7246, -98.7117,\n",
      "        -98.6987, -98.6856, -98.6723, -98.6589, -98.6453, -98.6316, -98.6178,\n",
      "        -98.6039, -98.5897, -98.5755, -98.5611, -98.5466, -98.5319, -98.5171,\n",
      "        -98.5021, -98.4870, -98.4717, -98.4562, -98.4406, -98.4249, -98.4090,\n",
      "        -98.3929, -98.3767, -98.3603, -98.3437, -98.3270, -98.3101, -98.2930,\n",
      "        -98.2758, -98.2583, -98.2407, -98.2230, -98.2050, -98.1869, -98.1686,\n",
      "        -98.1501, -98.1314, -98.1125, -98.0934, -98.0742, -98.0547, -98.0351,\n",
      "        -98.0152, -97.9952, -97.9749, -97.9545, -97.9338, -97.9129, -97.8919,\n",
      "        -97.8706, -97.8491, -97.8273, -97.8054, -97.7832, -97.7608, -97.7382,\n",
      "        -97.7153, -97.6923, -97.6690, -97.6454, -97.6216, -97.5976, -97.5733,\n",
      "        -97.5488, -97.5241, -97.4991, -97.4738, -97.4483, -97.4225, -97.3965,\n",
      "        -97.3702, -97.3436, -97.3168, -97.2897, -97.2623, -97.2346, -97.2067,\n",
      "        -97.1785, -97.1500, -97.1212, -97.0921, -97.0627, -97.0331, -97.0031,\n",
      "        -96.9728, -96.9423, -96.9114, -96.8802, -96.8487, -96.8168, -96.7847,\n",
      "        -96.7522, -96.7194, -96.6862, -96.6528, -96.6190, -96.5848, -96.5503,\n",
      "        -96.5155, -96.4803, -96.4447, -96.4088, -96.3725, -96.3359, -96.2989,\n",
      "        -96.2615, -96.2237, -96.1856, -96.1470, -96.1081, -96.0688, -96.0291,\n",
      "        -95.9890, -95.9485, -95.9075, -95.8662, -95.8245, -95.7823, -95.7397,\n",
      "        -95.6966, -95.6532, -95.6093, -95.5649, -95.5201, -95.4749, -95.4292,\n",
      "        -95.3830, -95.3363, -95.2892, -95.2416, -95.1936, -95.1450, -95.0960,\n",
      "        -95.0464, -94.9964, -94.9459, -94.8948, -94.8432, -94.7912, -94.7385,\n",
      "        -94.6854, -94.6317, -94.5775, -94.5227, -94.4674, -94.4115, -94.3551,\n",
      "        -94.2980, -94.2404, -94.1823, -94.1235, -94.0641, -94.0042, -93.9436,\n",
      "        -93.8824, -93.8206, -93.7582, -93.6952, -93.6315, -93.5672, -93.5022,\n",
      "        -93.4365, -93.3702, -93.3033, -93.2356, -93.1673, -93.0983, -93.0286,\n",
      "        -92.9581, -92.8870, -92.8152, -92.7426, -92.6693, -92.5952, -92.5204,\n",
      "        -92.4449, -92.3686, -92.2915, -92.2136, -92.1350, -92.0555, -91.9753,\n",
      "        -91.8942, -91.8123, -91.7296, -91.6461, -91.5617, -91.4765, -91.3904,\n",
      "        -91.3034, -91.2156, -91.1268, -91.0372, -90.9467, -90.8552, -90.7628,\n",
      "        -90.6695, -90.5753, -90.4801, -90.3839, -90.2868, -90.1887, -90.0896,\n",
      "        -89.9895, -89.8884, -89.7862, -89.6830, -89.5788, -89.4736, -89.3672,\n",
      "        -89.2598, -89.1513, -89.0418, -88.9311, -88.8193, -88.7063, -88.5923,\n",
      "        -88.4770, -88.3606, -88.2431, -88.1243, -88.0043, -87.8832, -87.7608,\n",
      "        -87.6372, -87.5123, -87.3861, -87.2587, -87.1300, -87.0000, -86.8687,\n",
      "        -86.7361, -86.6021, -86.4668, -86.3301, -86.1920, -86.0525, -85.9116,\n",
      "        -85.7693, -85.6256, -85.4804, -85.3337, -85.1856, -85.0359, -84.8848,\n",
      "        -84.7321, -84.5779, -84.4221, -84.2647, -84.1058, -83.9452, -83.7831,\n",
      "        -83.6193, -83.4538, -83.2867, -83.1178, -82.9473, -82.7751, -82.6011,\n",
      "        -82.4253, -82.2478, -82.0685, -81.8874, -81.7044, -81.5196, -81.3329,\n",
      "        -81.1444, -80.9539, -80.7615, -80.5672, -80.3709, -80.1726, -79.9723,\n",
      "        -79.7700, -79.5657, -79.3593, -79.1508, -78.9402, -78.7275, -78.5126,\n",
      "        -78.2955, -78.0763, -77.8549, -77.6312, -77.4052, -77.1770, -76.9464,\n",
      "        -76.7136, -76.4784, -76.2408, -76.0008, -75.7584, -75.5135, -75.2662,\n",
      "        -75.0163, -74.7640, -74.5091, -74.2516, -73.9915, -73.7288, -73.4634,\n",
      "        -73.1954, -72.9246, -72.6511, -72.3749, -72.0958, -71.8140, -71.5293,\n",
      "        -71.2417, -70.9512, -70.6578, -70.3614, -70.0620, -69.7596, -69.4541,\n",
      "        -69.1456, -68.8339, -68.5191, -68.2011, -67.8799, -67.5555, -67.2277,\n",
      "        -66.8967, -66.5623, -66.2246, -65.8834, -65.5388, -65.1907, -64.8391,\n",
      "        -64.4839, -64.1252, -63.7628, -63.3968, -63.0271, -62.6536, -62.2764,\n",
      "        -61.8953, -61.5104, -61.1216, -60.7289, -60.3322, -59.9316, -59.5268,\n",
      "        -59.1180, -58.7051, -58.2879, -57.8666, -57.4410, -57.0111, -56.5769,\n",
      "        -56.1383, -55.6952, -55.2477, -54.7957, -54.3391, -53.8778, -53.4119,\n",
      "        -52.9414, -52.4660, -51.9859, -51.5009, -51.0110, -50.5162, -50.0163,\n",
      "        -49.5114, -49.0014, -48.4863, -47.9660, -47.4404, -46.9095, -46.3732,\n",
      "        -45.8315, -45.2843, -44.7317, -44.1734, -43.6095, -43.0399, -42.4645,\n",
      "        -41.8834, -41.2963, -40.7034, -40.1044, -39.4994, -38.8883, -38.2710,\n",
      "        -37.6475, -37.0176, -36.3815, -35.7388, -35.0897, -34.4341, -33.7718,\n",
      "        -33.1028, -32.4271, -31.7445, -31.0551, -30.3587, -29.6552, -28.9447,\n",
      "        -28.2269, -27.5020, -26.7697, -26.0300, -25.2828, -24.5281, -23.7657,\n",
      "        -22.9957, -22.2179, -21.4322, -20.6386, -19.8369, -19.0272, -18.2093,\n",
      "        -17.3831, -16.5486, -15.7057, -14.8542, -13.9942, -13.1254, -12.2479,\n",
      "        -11.3615, -10.4662,  -9.5618,  -8.6483,  -7.7255,  -6.7935,  -5.8520,\n",
      "         -4.9010,  -3.9404,  -2.9701,  -1.9900,  -1.0000,   0.0000])\n"
     ]
    }
   ],
   "source": [
    "def discount_rewards(reward_array, DISCOUNT = 0.99):\n",
    "    '''\n",
    "    This will take the reward, and recursively update them, in reverse.\n",
    "    \n",
    "    input: reward_array[list]: list of rewards that are generated per episode\n",
    "    \n",
    "    '''\n",
    "    \n",
    "#     reward_array = torch.FloatTensor(reward_array)\n",
    "    G = torch.zeros((len(reward_array),)).float()\n",
    "    cumulative = 0\n",
    "    for i in reversed(range(len(reward_array)-1)):\n",
    "        G[i] = reward_array[i+1] + DISCOUNT * G[i+1]\n",
    "    return G\n",
    "\n",
    "# Create the discounted goal at each step\n",
    "\n",
    "G = discount_rewards(reward_list)\n",
    "print(G)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss function. The policy part - is the output a probability or the action it represents\n",
    "# action_prob_val = torch.tensor(action_prob_val).float()\n",
    "# log_probs = -G * action_prob_val\n",
    "\n",
    "\n",
    "\n",
    "log_probs = -torch.sum(G) # This will return nan for negative number! wtf. B/c I forgot what a log graph looks like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import evaluate\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "# importlib.reload(evaluation)\n",
    "\n",
    "\n",
    "\n",
    "def generate_episode(env, policy_estimator):\n",
    "    '''\n",
    "    Useful for reinforcementl earning tasks with Monte Carlo tasks. This function is specifcally for policy gradient methods\n",
    "    \n",
    "    Input: env: Environment\n",
    "    policy_estimator: function approximator. Specifically Neural network\n",
    "    '''\n",
    "    # Initial state\n",
    "    \n",
    "    \n",
    "    # Record actions, states, and rewards for each episode\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "\n",
    "\n",
    "\n",
    "    # If it isn't the x episode, then continue the evaluation\n",
    "\n",
    "    s_0 = torch.tensor(env.reset()).float()\n",
    "\n",
    "    while done is False:\n",
    "\n",
    "        action = action_choice(policy_estimator, s_0)\n",
    "        s_1, reward, done, _ = env.step(action)\n",
    "\n",
    "\n",
    "        states.append(s_1)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        s_1 = torch.from_numpy(s_1).float()\n",
    "        s_0 = s_1 # I missed this, and this is really important!!!!!\n",
    "        \n",
    "        if done:\n",
    "            return states, actions, rewards\n",
    "    \n",
    "def action_choice(policy_estimator, state):\n",
    "    # May cause conflicts with the Maze environment\n",
    "    state = state.unsqueeze(0)[0]\n",
    "    action_probs = policy_estimator(state).detach().numpy()\n",
    "    action = np.random.choice((policy_estimator.n_output), p = action_probs)\n",
    "    return action\n",
    "    \n",
    "def discount_rewards(reward_array, DISCOUNT = 0.99):\n",
    "    '''\n",
    "    This will take the reward, and recursively update them, in reverse.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "#     reward_array = torch.FloatTensor(reward_array)\n",
    "    G = torch.zeros((len(reward_array),)).float()\n",
    "    cumulative = 0\n",
    "    for i in reversed(range(len(reward_array)-1)):\n",
    "        G[i] = reward_array[i+1] + DISCOUNT * G[i+1]\n",
    "    return G\n",
    "\n",
    "def reinforce(env, policy_estimator, DISCOUNT = 0.99, EPISODES = 2000, lr = 0.01, EVALUATION_STEP = 400):\n",
    "    # Define optimizer\n",
    "    optimizer = optim.Adam(policy_estimator.parameters(), lr = lr)\n",
    "\n",
    "    cumulative_reward = []\n",
    "    running_reward = 0\n",
    "    \n",
    "    steps = []\n",
    "    eval_reward = []\n",
    "        \n",
    "    \n",
    "    # Update over X number of episodes (default 2000)\n",
    "    for ep in range(EPISODES):\n",
    "        \n",
    "        # Evaluate every Y episodes (default 400). Add reward step each iteration\n",
    "        if ep % EVALUATION_STEP == 0:\n",
    "            st, er = evaluate(env, Q_table = None, step_bound = 100, num_itr = 10, Gym = True, policy_estimator = policy_estimator)\n",
    "            steps.append(st)\n",
    "            eval_reward.append(er)\n",
    "\n",
    "        # Generate episode, save all states inside the episode\n",
    "        states, actions, rewards = generate_episode(env, policy_estimator)\n",
    "        \n",
    "        # Record running reward\n",
    "        running_reward += sum(rewards)\n",
    "        cumulative_reward.append(running_reward)\n",
    "        \n",
    "        \n",
    "        # Create discounted reward array for each iteration of episode\n",
    "        G = discount_rewards(rewards, DISCOUNT)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Convert arrays to tensor form\n",
    "        \n",
    "        \n",
    "        action_tensor = torch.LongTensor(actions)\n",
    "        states_tensor = torch.tensor(states).float()\n",
    "        reward_tensor = torch.tensor(G).float()\n",
    "\n",
    "        \n",
    "\n",
    "# #############################################\n",
    "#         Baseline##########\n",
    "#         WINDOW = 20\n",
    "#         baseline = np.mean(reward_tensor[-WINDOW:].detach().numpy())\n",
    "#         reward_with_baseline = reward_tensor - baseline\n",
    "#           selected_logprobs = reward_with_baseline * logprob[np.arange(len(action_tensor)), action_tensor]\n",
    "# #############################################      \n",
    "        \n",
    "    # Find the loss function\n",
    "        logprob = torch.log(policy_estimator(states_tensor))\n",
    "        selected_logprobs = reward_tensor * logprob[np.arange(len(action_tensor)), action_tensor]\n",
    "        loss = -selected_logprobs.sum()\n",
    "        \n",
    "        print(\"\\r Ep {}/{} running reward: {}, loss {}\".format(ep, EPISODES, running_reward, loss), end = \"\")\n",
    "        \n",
    "        #Backprop & update step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Clear values for new episode\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "\n",
    "    return cumulative_reward, steps, eval_reward\n",
    "                \n",
    "                \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "object too deep for desired array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-057fd0ed0efb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CartPole-v0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPolicyEstimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcum_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreinforce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcum_reward\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-aa0f6946d85d>\u001b[0m in \u001b[0;36mreinforce\u001b[1;34m(env, policy_estimator, DISCOUNT, EPISODES, lr, EVALUATION_STEP)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;31m# Evaluate every Y episodes (default 400). Add reward step each iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mEVALUATION_STEP\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[0mst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_bound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_itr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGym\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_estimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             \u001b[0msteps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0meval_reward\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\dev\\AutoSys\\Reinforcement_Learning_VI_QL\\evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(env, Q_table, step_bound, num_itr, Gym, policy_estimator)\u001b[0m\n\u001b[0;32m     66\u001b[0m                                         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m                                         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_action_policy_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_estimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m                                         \u001b[0mstate_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\dev\\AutoSys\\Reinforcement_Learning_VI_QL\\evaluation.py\u001b[0m in \u001b[0;36mget_action_policy_grad\u001b[1;34m(policy_estimator, state)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0maction_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: object too deep for desired array"
     ]
    }
   ],
   "source": [
    "# Cumulative reward\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "net = PolicyEstimator(env)\n",
    "cum_reward = reinforce(env, net)\n",
    "print(cum_reward[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 different seeds\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(5,1, figsize = (12,40))\n",
    "for run in range(5):\n",
    "    # Create a new environment instance (seed is different at each run)\n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "    env._max_episode_steps = 1000\n",
    "    seed = env.seed()\n",
    "    pe = policy_estimator(env)\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "    rewards = reinforce(env, pe)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'time of run {run} for seed {seed} in minutes: {(end-start) / 60.}')\n",
    "    window = 10\n",
    "    smoothed_rewards = [np.mean(rewards[i-window:i+1]) if i > window \n",
    "                        else np.mean(rewards[:i+1]) for i in range(len(rewards))]\n",
    "    \n",
    "    axs[run].plot(rewards)\n",
    "    axs[run].plot(smoothed_rewards)\n",
    "    axs[run].set_ylabel('Total Rewards')\n",
    "    axs[run].set_xlabel('Episodes')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "fig, axs = plt.subplots(1,1,figsize= (15,11))\n",
    "seeds = []\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "for run in range(5):\n",
    "    env = gym.make(\"Acrobot-v1\")\n",
    "#     env._max_episode_steps = 1000\n",
    "    seed = env.seed()\n",
    "    seeds.append(seed)\n",
    "    pe = PolicyEstimator(env)\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "    cum_reward = reinforce(env, pe)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'time of run {run} for seed {seed} in minutes: {(end-start) / 60.}')\n",
    "    axs.plot(cum_reward)\n",
    "    axs.set_ylabel(\"Cumulative reward\")\n",
    "    axs.set_xlabel(\"Episodes\")\n",
    "    axs.legend(seeds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt with class evaluation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RUNS = 2 # number of different seeds you are trying out - checking by eye for variance\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(RUNS,1,figsize= (15 * RUNS,11))\n",
    "seeds = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for run in range(RUNS):\n",
    "    env = gym.make(\"Acrobot-v1\")\n",
    "#     env._max_episode_steps = 1000\n",
    "    seed = env.seed()\n",
    "    seeds.append(seed)\n",
    "    pe = PolicyEstimator(env)\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "    cum_reward, steps, eval_reward = reinforce(env, pe, EPISODES = 2000)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'time of run {run} for seed {seed} in minutes: {(end-start) / 60.}')\n",
    "    axs[0].plot(steps)\n",
    "    axs[0].set_ylabel(\"Steps taken\")\n",
    "    axs[0].set_xlabel(\"Episodes\")\n",
    "    axs[0].legend(seeds)\n",
    "    \n",
    "    print(f'time of run {run} for seed {seed} in minutes: {(end-start) / 60.}')\n",
    "    axs[1].plot(eval_reward)\n",
    "    axs[1].set_ylabel(\"Cumulative reward\")\n",
    "    axs[1].set_xlabel(\"Episodes\")\n",
    "    axs[1].legend(seeds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(steps)\n",
    "print(eval_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.])\n",
      "tensor([0.2792, 0.3435, 0.1829, 0.1944], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "from maze import *\n",
    "import gym\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import evaluation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "class MDP(Maze):\n",
    "    '''\n",
    "    Inherits all the attributes from the maze\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.LEARNING_RATE = 0.1\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "class PolicyEstimator(nn.Module):\n",
    "    '''\n",
    "    Feedforward network definition for maze environment\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, env):\n",
    "        super(PolicyEstimator,self).__init__()\n",
    "\n",
    "        self.n_input = 1\n",
    "        self.n_output = env.anum\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(self.n_input, 16)\n",
    "        self.fc2 = nn.Linear(16, self.n_output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim = -1)\n",
    "        return x        \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "env = MDP()\n",
    "\n",
    "state = torch.tensor(env.reset()).float().unsqueeze(0) # unsqueeze adds a fake batch dimension\n",
    "print(state)\n",
    "net = PolicyEstimator(env)\n",
    "output = net(state)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "env = MDP()\n",
    "print(type(env.reset()))\n",
    "env2 = gym.make(\"CartPole-v0\")\n",
    "print(type(env2.reset()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluate(env, policy_estimator= net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNS = 2\n",
    "env = \n",
    "for run in range(RUNS):\n",
    "#     env = gym.make(\"Acrobot-v1\")\n",
    "#     env._max_episode_steps = 1000\n",
    "#     seed = env.seed()\n",
    "#     seeds.append(seed)\n",
    "    pe = PolicyEstimator(env)\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "    cum_reward, steps, eval_reward = reinforce(env, pe, EPISODES= 2000)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'time of run {run} for seed {seed} in minutes: {(end-start) / 60.}')\n",
    "    axs[0].plot(steps)\n",
    "    axs[0].set_ylabel(\"Steps taken\")\n",
    "    axs[0].set_xlabel(\"Episodes\")\n",
    "#     axs[0].legend(seeds)\n",
    "    \n",
    "    print(f'time of run {run} for seed {seed} in minutes: {(end-start) / 60.}')\n",
    "    axs[1].plot(eval_reward)\n",
    "    axs[1].set_ylabel(\"Cumulative reward\")\n",
    "    axs[1].set_xlabel(\"Episodes\")\n",
    "#     axs[1].legend(seeds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
